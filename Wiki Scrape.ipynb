{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\haedm\\anaconda3\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\haedm\\anaconda3\\lib\\site-packages (from requests) (2.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\haedm\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haedm\\anaconda3\\lib\\site-packages (from requests) (2018.4.16)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\haedm\\anaconda3\\lib\\site-packages (from requests) (1.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\haedm\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\haedm\\anaconda3\\lib\\site-packages (4.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\haedm\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elementpath in c:\\users\\haedm\\anaconda3\\lib\\site-packages (2.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\haedm\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\haedm\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\haedm\\anaconda3\\lib\\site-packages (from wikipedia) (2.25.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\haedm\\anaconda3\\lib\\site-packages (from wikipedia) (4.6.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\haedm\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haedm\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2018.4.16)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\haedm\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\haedm\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\haedm\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install elementpath\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4 \n",
    "from lxml import etree, html\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Wiki Infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_search(text, capture_v1, capture_v1b, capture_v2, append_list):\n",
    "    if 'Country' in text:\n",
    "        if '</li>' in text:\n",
    "            number = len(re.findall(capture_v1, text))\n",
    "            for capture_no in range(number):\n",
    "                new_text = str(re.findall(capture_v1, text)[capture_no])\n",
    "                f = re.findall(capture_v2, new_text)[0]\n",
    "                if '<br/>' in f:\n",
    "                    f = f.replace('<br/>', ' ')\n",
    "                append_list.append(f)\n",
    "        else:\n",
    "            f = re.findall(capture_v1b, text)[0]\n",
    "            if '<br/>' in f:\n",
    "                f = f.replace('<br/>', ' ')\n",
    "            append_list.append(f)\n",
    "    elif '</li>' in text:\n",
    "        number = len(re.findall(capture_v1, text))\n",
    "        for capture_no in range(number):\n",
    "            new_text = str(re.findall(capture_v1, text)[capture_no])\n",
    "            if 'href' in new_text:\n",
    "                f = re.findall(capture_v2, new_text)[0]\n",
    "                if '<br/>' in f:\n",
    "                    f = f.replace('<br/>', ' ')\n",
    "                append_list.append(f)\n",
    "            else:\n",
    "                f = new_text\n",
    "                if '<br/>' in f:\n",
    "                    f = f.replace('<br/>', ' ')\n",
    "                append_list.append(f)\n",
    "    else:\n",
    "        if 'href' in text:\n",
    "            f = re.findall(capture_v2, text)[0]\n",
    "            if '<br/>' in f:\n",
    "                f = f.replace('<br/>', ' ')\n",
    "            append_list.append(f)\n",
    "        else:\n",
    "            f = re.findall(capture_v1b, text)[0]\n",
    "            if '<br/>' in f:\n",
    "                f = f.replace('<br/>', ' ')\n",
    "            append_list.append(f)\n",
    "    return(append_list)\n",
    "\n",
    "def date_search(text, capture_v1, capture_v2, capture_v3):\n",
    "    found = 0\n",
    "    if '</li>' in text:\n",
    "        number = len(re.findall(capture_v1, text))\n",
    "        for capture_no in range(number):\n",
    "            new_text = str(re.findall(capture_v1, text)[capture_no])\n",
    "            if 'United States' in new_text or 'Film Festival' in new_text:\n",
    "                f = re.findall(capture_v2, new_text)[0]\n",
    "                if '<br/>' in f:\n",
    "                    f = f.replace('<br/>', ' ')\n",
    "                found = 1\n",
    "            elif found == 0:\n",
    "                f = re.findall(capture_v2, new_text)[0]\n",
    "                if '<br/>' in f:\n",
    "                    f = f.replace('<br/>', ' ')\n",
    "    return(f)\n",
    "\n",
    "def time_search(text, capture_v1):\n",
    "    f = re.findall(capture_v1, text)[0]\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_data_scrape(movie_name, soup, movie_url):\n",
    "\n",
    "    # getting infobox\n",
    "    infobox = soup.find('table', {'class': 'infobox'})\n",
    "\n",
    "    #regex for value_captures\n",
    "    capture_v1 = r'<li>(.*?)</li>' #no linked value\n",
    "    capture_v1b= r'<td>(.*?)</td>' #no linked value\n",
    "    capture_v2 = r'href=\\\".*?\\\" title=.*?>(.*?)</a>' #linked value\n",
    "    capture_v3 = r'([0-9]{4}-[0-9]{2}-[0-9]{2})' #date 2020-12-25\n",
    "    capture_v3b = r'(W* [0-9]{1,2}, [0-9]{4})' #date v2 April 13, 2012\n",
    "    capture_v4 = r'([0-9]*) minute' #run time\n",
    "    capture_v5 = r'<td>(.*?)<' #country/language\n",
    "    capture_v6 = r'(.*?)<' #country with multiples\n",
    "    capture_v7 = r'<td>(.*lion?)<'\n",
    "\n",
    "\n",
    "    #search variables/lists\n",
    "    directors   = []\n",
    "    producers   = []\n",
    "    screen_play = []\n",
    "    based_on    = 0\n",
    "    starring    = []\n",
    "    cinemat     = []\n",
    "    production  = []\n",
    "    distribute  = []\n",
    "    release_date = pd.datetime(2099, 1, 1)\n",
    "    run_time    = -1\n",
    "    country     = []\n",
    "    language    = []\n",
    "    budget      = np.nan\n",
    "    box_office  = np.nan\n",
    "\n",
    "    #loop through infobox on wiki\n",
    "    for i in range(0, len(infobox.find_all('tr'))):\n",
    "        text = str(infobox.find_all('tr')[i])\n",
    "\n",
    "        if 'Directed by' in text:\n",
    "            try:\n",
    "                directors = value_search(text, capture_v1, capture_v1b, capture_v2, directors)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Produced by' in text:\n",
    "            try:\n",
    "                producers = value_search(text, capture_v1, capture_v1b, capture_v2, producers)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Screenplay by' in text or 'Story by' in text or 'Written by' in text:\n",
    "            try:\n",
    "                screen_play = value_search(text, capture_v1, capture_v1b, capture_v2, screen_play)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Based on' in text:\n",
    "            based_on = 1\n",
    "\n",
    "        elif 'Starring' in text:\n",
    "            try:\n",
    "                starring = value_search(text, capture_v1, capture_v1b, capture_v2, starring)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Cinematography' in text:\n",
    "            try:\n",
    "                cinemat = value_search(text, capture_v1, capture_v1b, capture_v2, cinemat)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Production' in text:\n",
    "            try:\n",
    "                production = value_search(text, capture_v1, capture_v1b, capture_v2, production)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Distributed by' in text:\n",
    "            try:\n",
    "                distribute = value_search(text, capture_v1, capture_v1b, capture_v2, distribute)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Release date' in text or 'release' in text:\n",
    "            try:\n",
    "                release_date = date_search(text, capture_v1, capture_v3, capture_v3b)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Running time' in text:\n",
    "            try:\n",
    "                run_time = time_search(text, capture_v4)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Country' in text:\n",
    "            try:\n",
    "                country = value_search(text, capture_v1, capture_v5, capture_v6, country)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Language' in text:\n",
    "            try:\n",
    "                language = value_search(text, capture_v1, capture_v1b, capture_v5, language)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Budget' in text:\n",
    "            temp_list = []\n",
    "            try:\n",
    "                budget = value_search(text, capture_v1, capture_v1b, capture_v5, temp_list)[0]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Box office' in text:\n",
    "            temp_list = []\n",
    "            try:\n",
    "                box_office = value_search(text, capture_v1, capture_v1b, capture_v5, temp_list)[0]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return_list = [movie_name, movie_url, directors, producers, screen_play, based_on, starring, cinemat, production, distribute,\n",
    "           release_date, run_time, country, language, budget, box_office]\n",
    "    return(return_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Movie Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\haedm\\Downloads\\Movies.csv')\n",
    "df_final = pd.read_csv(r'C:\\Users\\haedm\\Downloads\\Movies_New.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Wiki-Scraping and Re-Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Big Hero 6 - 2014\n",
      "https://en.wikipedia.org/wiki/Big_Hero_6_(film)\n",
      "\n",
      "Bombshell - 2019\n",
      "https://en.wikipedia.org/wiki/Bombshell_(2019_film)\n",
      "\n",
      "The Farewell - 2019\n",
      "https://en.wikipedia.org/wiki/The_Farewell_(2019_film)\n",
      "\n",
      "The Grand Budapest Hotel - 2014\n",
      "https://en.wikipedia.org/wiki/The_Grand_Budapest_Hotel\n",
      "\n",
      "Jumanji: The Next Level - 2019\n",
      "https://en.wikipedia.org/wiki/Jumanji:_The_Next_Level\n",
      "\n",
      "Uncut Gems - 2019\n",
      "https://en.wikipedia.org/wiki/Uncut_Gems\n",
      "\n",
      "Zootopia - 2016\n",
      "https://en.wikipedia.org/wiki/Zootopia\n",
      "\n",
      "Marriage Story - 2019\n",
      "https://en.wikipedia.org/wiki/Marriage_Story\n",
      "\n",
      "Underwater - 2020\n",
      "https://en.wikipedia.org/wiki/Underwater_(film)\n",
      "\n",
      "Parasite - 2019\n",
      "https://en.wikipedia.org/wiki/Parasite_(2019_film)\n",
      "\n",
      "1917 - 2019\n",
      "https://en.wikipedia.org/wiki/1917_(2019_film)\n",
      "\n",
      "Weathering with You - 2019\n",
      "https://en.wikipedia.org/wiki/Weathering_with_You\n",
      "\n",
      "The Gentlemen - 2020\n",
      "https://en.wikipedia.org/wiki/The_Gentlemen_(2019_film)\n",
      "\n",
      "The Rhythm Section - 2020\n",
      "https://en.wikipedia.org/wiki/The_Rhythm_Section\n",
      "\n",
      "Birds of Prey - 2020\n",
      "https://en.wikipedia.org/wiki/Birds_of_Prey_(2020_film)\n",
      "\n",
      "Toy Story 4 - 2019\n",
      "https://en.wikipedia.org/wiki/Toy_Story_4\n",
      "\n",
      "Midsommar - 2019\n",
      "https://en.wikipedia.org/wiki/Midsommar_(film)\n",
      "\n",
      "Sonic the Hedgehog - 2020\n",
      "https://en.wikipedia.org/wiki/Sonic_the_Hedgehog_(film)\n",
      "\n",
      "Fantasy Island - 2020\n",
      "https://en.wikipedia.org/wiki/Fantasy_Island_(film)\n",
      "\n",
      "Luce - 2019\n",
      "https://en.wikipedia.org/wiki/Luce_(film)\n",
      "\n",
      "Call of the Wild - 2020\n",
      "https://en.wikipedia.org/wiki/The_Call_of_the_Wild_(2020_film)\n",
      "\n",
      "Downhill - 2020\n",
      "https://en.wikipedia.org/wiki/Downhill_(2020_film)\n",
      "\n",
      "The Invisible Man - 2020\n",
      "https://en.wikipedia.org/wiki/The_Invisible_Man_(2020_film)\n",
      "\n",
      "Hot Rod - 2007\n",
      "https://en.wikipedia.org/wiki/Hot_Rod_(2007_film)\n",
      "\n",
      "Drive - 2011\n",
      "https://en.wikipedia.org/wiki/Drive_(2011_film)\n",
      "\n",
      "The Lodge - 2020\n",
      "https://en.wikipedia.org/wiki/The_Lodge_(film)\n",
      "\n",
      "Onward - 2020\n",
      "https://en.wikipedia.org/wiki/Onward_(film)\n",
      "\n",
      "Emma - 2020\n",
      "https://en.wikipedia.org/wiki/Emma_(2020_film)\n",
      "\n",
      "Portrait of a Lady on Fire - 2019\n",
      "https://en.wikipedia.org/wiki/Portrait_of_a_Lady_on_Fire\n",
      "\n",
      "The Hunt - 2020\n",
      "https://en.wikipedia.org/wiki/The_Hunt_(2020_film)\n",
      "\n",
      "Fight Club - 1999\n",
      "https://en.wikipedia.org/wiki/Fight_Club\n",
      "\n",
      "I Still Believe - 2020\n",
      "https://en.wikipedia.org/wiki/I_Still_Believe_(film)\n",
      "\n",
      "Long Shot - 2019\n",
      "https://en.wikipedia.org/wiki/Long_Shot_(2019_film)\n",
      "\n",
      "Star Wars IV: A New Hope - 1977\n",
      "https://en.wikipedia.org/wiki/Star_Wars_(film)\n",
      "\n",
      "Clue - 1985\n",
      "https://en.wikipedia.org/wiki/Clue_(film)\n",
      "\n",
      "Groundhog Day - 1993\n",
      "https://en.wikipedia.org/wiki/Groundhog_Day_(film)\n",
      "\n",
      "Inception - 2010\n",
      "https://en.wikipedia.org/wiki/Inception\n",
      "\n",
      "Blade Runner - 1982\n",
      "https://en.wikipedia.org/wiki/Blade_Runner\n",
      "\n",
      "Free Solo - 2018\n",
      "https://en.wikipedia.org/wiki/Free_Solo\n",
      "\n",
      "Father of the Bride - 1991\n",
      "https://en.wikipedia.org/wiki/Father_of_the_Bride_(1991_film)\n",
      "\n",
      "The Platform - 2020\n",
      "https://en.wikipedia.org/wiki/The_Platform_(film)\n",
      "\n",
      "Devil Wears Prada - 2006\n",
      "https://en.wikipedia.org/wiki/The_Devil_Wears_Prada_(film)\n",
      "\n",
      "Wall-E - 2008\n",
      "https://en.wikipedia.org/wiki/WALL-E\n",
      "\n",
      "Rosemary's Baby - 1968\n",
      "https://en.wikipedia.org/wiki/Rosemary%27s_Baby_(film)\n",
      "\n",
      "No Crying at the Dinner Table - 2020\n",
      "https://en.wikipedia.org/wiki/No_Crying_at_the_Dinner_Table\n",
      "\n",
      "Treasure Planet - 2002\n",
      "https://en.wikipedia.org/wiki/Treasure_Planet\n",
      "\n",
      "My Darling Vivian - 2020\n",
      "https://en.wikipedia.org/wiki/My_Darling_Vivian\n",
      "\n",
      "Le Choc du Futur - 2020\n",
      "https://en.wikipedia.org/wiki/Le_choc\n",
      "\n",
      "Snowpiercer - 2013\n"
     ]
    }
   ],
   "source": [
    "# movie_list = ['Charlie and the chocolate factory', 'home alone', 'the little mermaid']\n",
    "\n",
    "new_df      = df[['Date Seen', 'Title', 'Zach Rating', 'Hayley Rating']].copy()   \n",
    "temp_df     = pd.DataFrame(columns=['Title', 'Movie_URL', 'Directors','Producers','Story_and_ScreenPlay_Writers','BasedOn_Ind',\n",
    "                                    'Starring','Cinematographers','ProductionCompany','DistributionCompany',\n",
    "                                    'ReleaseDate','RunTime_Min','Country','Language','Budget','BoxOffice'\n",
    "                                   ])\n",
    "no_matches = []\n",
    "    \n",
    "#loop through movies\n",
    "for movie_name in df.Title:\n",
    "#     if movie_name in df_final.Title.unique():\n",
    "#         continue\n",
    "    if str(list(df.loc[df.Title == movie_name]['Release'])[0]) == 'nan':\n",
    "        print('{} - no release year'.format(movie_name))\n",
    "        no_matches.append(movie_name)\n",
    "        continue\n",
    "    else:\n",
    "        movie_release = str(list(df.loc[df.Title == movie_name]['Release'])[0])\n",
    "        movie_year = datetime.datetime.strptime(movie_release, '%m/%d/%Y').year\n",
    "        print(\"\\n{} - {}\".format(movie_name, movie_year))\n",
    "\n",
    "        #movie lookup and wiki-find\n",
    "        page_title = wikipedia.search(\"{} {} Film\".format(str(movie_name), str(movie_year)))[0]\n",
    "        if '&' in page_title:\n",
    "            page_title2 = page_title.replace('&', 'and')\n",
    "        else:\n",
    "            page_title2 = page_title\n",
    "        url = (\n",
    "            'https://en.wikipedia.org/w/api.php?action=query&prop=info&inprop=subjectid&titles={}&format=json'.format(page_title2))\n",
    "        json_response = requests.get(url).json()\n",
    "        wiki_page_id  = [page_id for page_id, page_info in json_response['query']['pages'].items()][0]\n",
    "\n",
    "        #pull wiki page\n",
    "        movie_title = np.nan\n",
    "        movie_id    = np.nan\n",
    "\n",
    "        try:\n",
    "            movie_title = wikipedia.page(page_title2, auto_suggest=False, redirect=True)\n",
    "        except:\n",
    "            movie_id = wikipedia.page(pageid=wiki_page_id)\n",
    "\n",
    "        try:\n",
    "            movie_id = wikipedia.page(pageid=wiki_page_id)\n",
    "        except:\n",
    "            movie_title = wikipedia.page(page_title2, auto_suggest=False, redirect=True)\n",
    "\n",
    "        if str(movie_title) != 'nan':\n",
    "            movie = movie_title\n",
    "        elif str(movie_id) != 'nan':\n",
    "            movie = movie_id\n",
    "        else:\n",
    "            print(\"can\\'t find url\")\n",
    "            continue\n",
    "\n",
    "        movie_url = movie.url\n",
    "        print(movie_url)\n",
    "\n",
    "        #scraping\n",
    "\n",
    "        # sending the request\n",
    "        response = requests.get(movie_url)\n",
    "\n",
    "        # parsing the response\n",
    "        soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "        if 'MOVIE' in str(soup).upper():\n",
    "            try:\n",
    "                output_list = wiki_data_scrape(movie_name, soup, movie_url)\n",
    "                temp_df.loc[len(temp_df)] = output_list\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            print('url not for movie!')\n",
    "\n",
    "#merge ratings df and scraped df\n",
    "new_df = pd.merge(new_df, temp_df, on='Title', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix URL\n",
    "movies=['The Hunger Games']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export csv\n",
    "new_df.to_csv(r'C:\\Users\\haedm\\Downloads\\Movies_New.csv')\n",
    "\n",
    "#import saved csv\n",
    "new_df = pd.read_csv(r'C:\\Users\\haedm\\Downloads\\Movies_New.csv', index_col='Unnamed: 0')\n",
    "    #convert string of list to string\n",
    "new_df.Directors = [ast.literal_eval(x) for x in new_df.Directors]\n",
    "new_df.Producers = [ast.literal_eval(x) for x in new_df.Producers]\n",
    "new_df.Story_and_ScreenPlay_Writers = [ast.literal_eval(x) for x in new_df.Story_and_ScreenPlay_Writers]\n",
    "new_df.Starring = [ast.literal_eval(x) for x in new_df.Starring]\n",
    "new_df.Cinematographers = [ast.literal_eval(x) for x in new_df.Cinematographers]\n",
    "new_df.ProductionCompany = [ast.literal_eval(x) for x in new_df.ProductionCompany]\n",
    "new_df.DistributionCompany = [ast.literal_eval(x) for x in new_df.DistributionCompany]\n",
    "new_df.Country = [ast.literal_eval(x) for x in new_df.Country]\n",
    "new_df.Language = [ast.literal_eval(x) for x in new_df.Language]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(new_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters and wiki notes\n",
    "def remove_special_characters_list(column):\n",
    "    master_list = []\n",
    "    for list_names in column:\n",
    "        temp_list = []\n",
    "        for name in list_names:\n",
    "            new_name = re.sub('\\(.*?\\)', '', name)\n",
    "            new_name = re.sub('\\<.*?\\>', '', new_name)\n",
    "            new_name = re.sub(\"[^0-9a-zA-Z\\. &+-]+\", \"\", new_name)\n",
    "            new_name = new_name.strip()\n",
    "            temp_list.append(new_name)\n",
    "        master_list.append(temp_list)\n",
    "    return master_list\n",
    "\n",
    "new_df.Directors = remove_special_characters_list(new_df.Directors)\n",
    "new_df.Producers = remove_special_characters_list(new_df.Producers)\n",
    "new_df.Story_and_ScreenPlay_Writers = remove_special_characters_list(new_df.Story_and_ScreenPlay_Writers)\n",
    "new_df.Starring = remove_special_characters_list(new_df.Starring)\n",
    "new_df.Cinematographers = remove_special_characters_list(new_df.Cinematographers)\n",
    "new_df.ProductionCompany = remove_special_characters_list(new_df.ProductionCompany)\n",
    "new_df.DistributionCompany = remove_special_characters_list(new_df.DistributionCompany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_money(column):\n",
    "    master_list = []\n",
    "    for item in column:\n",
    "        new_item = re.sub('\\(.*?\\)', '', str(item))\n",
    "        new_item = re.sub('\\<.*?\\>', '', new_item)\n",
    "        new_item = re.sub(\"[^₩$€¥©0-9a-zA-Z\\. -–]+\", \"\", new_item)\n",
    "        new_item = re.sub(\"US\", \"\", new_item)\n",
    "        new_item = re.sub(\"&lt;\", \"<\", new_item)\n",
    "        new_item = re.sub(\"&gt;\", \">\", new_item)\n",
    "        new_item = new_item.replace(u'\\xa0', ' ')\n",
    "        master_list.append(new_item)\n",
    "    return master_list\n",
    "\n",
    "new_df.Budget = remove_special_money(new_df.Budget)\n",
    "new_df.BoxOffice = remove_special_money(new_df.BoxOffice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting names that were combined into one item of list\n",
    "def remove_morphed_names(column):\n",
    "    master_list = []\n",
    "    for list_names in column:\n",
    "        temp_list = []\n",
    "        for name in list_names:\n",
    "            name = re.sub('  ', ' ', name)\n",
    "            count_spaces = sum(map(lambda x : 1 if ' ' in x else 0, name))  \n",
    "            if count_spaces in [0,1,2]:\n",
    "                temp_list.append(name)\n",
    "            elif count_spaces == 3:\n",
    "                temp = [x.start() for x in re.finditer(' ', name)]\n",
    "                name1 = name[0:temp[1]] \n",
    "                name2 = name[temp[1] + 1:]\n",
    "                temp_list.append(name1)\n",
    "                temp_list.append(name2)\n",
    "            elif count_spaces == 5:\n",
    "                temp = [x.start() for x in re.finditer(' ', name)]\n",
    "                name1 = name[0:temp[1]] \n",
    "                name2 = name[temp[1] + 1:temp[3]]\n",
    "                name3 = name[temp[3] + 1:]\n",
    "                temp_list.append(name1)\n",
    "                temp_list.append(name2)\n",
    "                temp_list.append(name3)\n",
    "            elif count_spaces == 7:\n",
    "                temp = [x.start() for x in re.finditer(' ', name)]\n",
    "                name1 = name[0:temp[1]] \n",
    "                name2 = name[temp[1] + 1:temp[3]]\n",
    "                name3 = name[temp[3] + 1:temp[5]]\n",
    "                name4 = name[temp[5] + 1:]\n",
    "                temp_list.append(name1)\n",
    "                temp_list.append(name2)\n",
    "                temp_list.append(name3)\n",
    "                temp_list.append(name4)\n",
    "            else:\n",
    "                temp_list.append(name)\n",
    "        master_list.append(temp_list)\n",
    "    return(master_list)\n",
    "\n",
    "##Directors\n",
    "new_df.Directors = remove_morphed_names(new_df.Directors)\n",
    "##Producers\n",
    "new_df.Producers = remove_morphed_names(new_df.Producers)\n",
    "##Writers\n",
    "new_df.Story_and_ScreenPlay_Writers = remove_morphed_names(new_df.Story_and_ScreenPlay_Writers)\n",
    "##Starring\n",
    "new_df.Starring = remove_morphed_names(new_df.Starring)\n",
    "##Cinematographers\n",
    "new_df.Cinematographers = remove_morphed_names(new_df.Cinematographers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vars = ['Director(s)', 'Producer(s)', 'Story / Screen Play Writer(s)', 'Starring', 'Cinematographer(s)',\n",
    "             'Production Company(s)', 'Distribution Company(s)', 'Country', 'Language']\n",
    "\n",
    "for list_var in list_vars:\n",
    "    temp = []\n",
    "    temp_df = pd.DataFrame(columns = ['value', 'count'])\n",
    "    print(\"\\n\\n{}\".format(list_var))\n",
    "    for i in new_df[list_var]:\n",
    "        for j in i:\n",
    "            temp.append(j)\n",
    "    values = pd.Series(temp).value_counts().index\n",
    "    counts = pd.Series(temp).value_counts().values\n",
    "    temp_df['value'] = values\n",
    "    temp_df['count'] = counts\n",
    "    display(temp_df.loc[temp_df['count'] == 1])\n",
    "#     print(pd.Series(temp).value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
