{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Wikipedia_Scrape.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Wikipedia_Scrape.py\n",
    "\n",
    "print('Importing Packages...')\n",
    "import requests\n",
    "import bs4 \n",
    "from lxml import etree, html\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import ast\n",
    "\n",
    "## Scraping Wiki Infobox\n",
    "def value_search(text, capture_v1, capture_v1b, capture_v2, append_list):\n",
    "    if 'Country' in text:\n",
    "        if '</li>' in text:\n",
    "            number = len(re.findall(capture_v1, text))\n",
    "            for capture_no in range(number):\n",
    "                new_text = str(re.findall(capture_v1, text)[capture_no])\n",
    "                f = re.findall(capture_v2, new_text)[0]\n",
    "                if '<br/>' in f:\n",
    "                    f = f.replace('<br/>', ' ')\n",
    "                append_list.append(f)\n",
    "        else:\n",
    "            f = re.findall(capture_v1b, text)[0]\n",
    "            if '<br/>' in f:\n",
    "                f = f.replace('<br/>', ' ')\n",
    "            append_list.append(f)\n",
    "    elif '</li>' in text:\n",
    "        number = len(re.findall(capture_v1, text))\n",
    "        for capture_no in range(number):\n",
    "            new_text = str(re.findall(capture_v1, text)[capture_no])\n",
    "            if 'href' in new_text:\n",
    "                f = re.findall(capture_v2, new_text)[0]\n",
    "                if '<br/>' in f:\n",
    "                    f = f.replace('<br/>', ' ')\n",
    "                append_list.append(f)\n",
    "            else:\n",
    "                f = new_text\n",
    "                if '<br/>' in f:\n",
    "                    f = f.replace('<br/>', ' ')\n",
    "                append_list.append(f)\n",
    "    else:\n",
    "        if 'href' in text:\n",
    "            f = re.findall(capture_v2, text)[0]\n",
    "            if '<br/>' in f:\n",
    "                f = f.replace('<br/>', ' ')\n",
    "            append_list.append(f)\n",
    "        else:\n",
    "            f = re.findall(capture_v1b, text)[0]\n",
    "            if '<br/>' in f:\n",
    "                f = f.replace('<br/>', ' ')\n",
    "            append_list.append(f)\n",
    "    return(append_list)\n",
    "\n",
    "def date_search(text, capture_v1, capture_v2, capture_v3):\n",
    "    found = 0\n",
    "    if '</li>' in text:\n",
    "        number = len(re.findall(capture_v1, text))\n",
    "        for capture_no in range(number):\n",
    "            new_text = str(re.findall(capture_v1, text)[capture_no])\n",
    "            if 'United States' in new_text or 'Film Festival' in new_text:\n",
    "                f = re.findall(capture_v2, new_text)[0]\n",
    "                if '<br/>' in f:\n",
    "                    f = f.replace('<br/>', ' ')\n",
    "                found = 1\n",
    "            elif found == 0:\n",
    "                f = re.findall(capture_v2, new_text)[0]\n",
    "                if '<br/>' in f:\n",
    "                    f = f.replace('<br/>', ' ')\n",
    "    return(f)\n",
    "\n",
    "def time_search(text, capture_v1):\n",
    "    f = re.findall(capture_v1, text)[0]\n",
    "    return(f)\n",
    "\n",
    "def wiki_data_scrape(movie_name, soup, movie_url):\n",
    "\n",
    "    # getting infobox\n",
    "    infobox = soup.find('table', {'class': 'infobox'})\n",
    "\n",
    "    #regex for value_captures\n",
    "    capture_v1 = r'<li>(.*?)</li>' #no linked value\n",
    "    capture_v1b= r'<td>(.*?)</td>' #no linked value\n",
    "    capture_v2 = r'href=\\\".*?\\\" title=.*?>(.*?)</a>' #linked value\n",
    "    capture_v3 = r'([0-9]{4}-[0-9]{2}-[0-9]{2})' #date 2020-12-25\n",
    "    capture_v3b = r'(W* [0-9]{1,2}, [0-9]{4})' #date v2 April 13, 2012\n",
    "    capture_v4 = r'([0-9]*) minute' #run time\n",
    "    capture_v5 = r'<td>(.*?)<' #country/language\n",
    "    capture_v6 = r'(.*?)<' #country with multiples\n",
    "    capture_v7 = r'<td>(.*lion?)<'\n",
    "\n",
    "\n",
    "    #search variables/lists\n",
    "    directors   = []\n",
    "    producers   = []\n",
    "    screen_play = []\n",
    "    based_on    = 0\n",
    "    starring    = []\n",
    "    cinemat     = []\n",
    "    production  = []\n",
    "    distribute  = []\n",
    "    release_date = pd.datetime(2099, 1, 1)\n",
    "    run_time    = -1\n",
    "    country     = []\n",
    "    language    = []\n",
    "    budget      = np.nan\n",
    "    box_office  = np.nan\n",
    "\n",
    "    #loop through infobox on wiki\n",
    "    for i in range(0, len(infobox.find_all('tr'))):\n",
    "        text = str(infobox.find_all('tr')[i])\n",
    "\n",
    "        if 'Directed by' in text:\n",
    "            try:\n",
    "                directors = value_search(text, capture_v1, capture_v1b, capture_v2, directors)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Produced by' in text:\n",
    "            try:\n",
    "                producers = value_search(text, capture_v1, capture_v1b, capture_v2, producers)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Screenplay by' in text or 'Story by' in text or 'Written by' in text:\n",
    "            try:\n",
    "                screen_play = value_search(text, capture_v1, capture_v1b, capture_v2, screen_play)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Based on' in text:\n",
    "            based_on = 1\n",
    "\n",
    "        elif 'Starring' in text:\n",
    "            try:\n",
    "                starring = value_search(text, capture_v1, capture_v1b, capture_v2, starring)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Cinematography' in text:\n",
    "            try:\n",
    "                cinemat = value_search(text, capture_v1, capture_v1b, capture_v2, cinemat)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Production' in text:\n",
    "            try:\n",
    "                production = value_search(text, capture_v1, capture_v1b, capture_v2, production)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Distributed by' in text:\n",
    "            try:\n",
    "                distribute = value_search(text, capture_v1, capture_v1b, capture_v2, distribute)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Release date' in text or 'release' in text:\n",
    "            try:\n",
    "                release_date = date_search(text, capture_v1, capture_v3, capture_v3b)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Running time' in text:\n",
    "            try:\n",
    "                run_time = time_search(text, capture_v4)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Country' in text:\n",
    "            try:\n",
    "                country = value_search(text, capture_v1, capture_v5, capture_v6, country)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Language' in text:\n",
    "            try:\n",
    "                language = value_search(text, capture_v1, capture_v1b, capture_v5, language)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Budget' in text:\n",
    "            temp_list = []\n",
    "            try:\n",
    "                budget = value_search(text, capture_v1, capture_v1b, capture_v5, temp_list)[0]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif 'Box office' in text:\n",
    "            temp_list = []\n",
    "            try:\n",
    "                box_office = value_search(text, capture_v1, capture_v1b, capture_v5, temp_list)[0]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return_list = [movie_name, movie_url, directors, producers, screen_play, based_on, starring, cinemat, production, distribute,\n",
    "           release_date, run_time, country, language, budget, box_office]\n",
    "    return(return_list)\n",
    "\n",
    "def scrape_wiki(df):\n",
    "    movie_list = ['Charlie and the chocolate factory', 'home alone', 'the little mermaid']\n",
    "\n",
    "    new_df      = df[['Date Seen', 'Title', 'Zach Rating', 'Hayley Rating']].copy()\n",
    "    temp_df = pd.DataFrame(columns=['Title', 'Movie_URL', 'Directors','Producers','Story_and_ScreenPlay_Writers','BasedOn_Ind',\n",
    "                                    'Starring','Cinematographers','ProductionCompany','DistributionCompany',\n",
    "                                    'ReleaseDate','RunTime_Min','Country','Language','Budget','BoxOffice'\n",
    "                                    ])\n",
    "    no_matches = []\n",
    "\n",
    "    #loop through movies\n",
    "    for movie_name in df.Title:\n",
    "        if movie_name in temp_df.Title:\n",
    "            continue\n",
    "        if str(list(df.loc[df.Title == movie_name]['Release'])[0]) == 'nan':\n",
    "            print('{} - no release year'.format(movie_name))\n",
    "            no_matches.append(movie_name)\n",
    "            continue\n",
    "        else:\n",
    "            movie_release = str(list(df.loc[df.Title == movie_name]['Release'])[0])\n",
    "            movie_year = datetime.datetime.strptime(movie_release, '%m/%d/%Y').year\n",
    "            print(\"\\n{} - {}\".format(movie_name, movie_year))\n",
    "\n",
    "            #Easy A Bug\n",
    "            if movie_name in ['Easy A']:\n",
    "                movie_name = \"\\\"Easy A\\\"\"\n",
    "\n",
    "            #movie lookup and wiki-find\n",
    "            page_title = wikipedia.search(\"{} {} Film\".format(str(movie_name), str(movie_year)))[0]\n",
    "            if '&' in page_title:\n",
    "                page_title2 = page_title.replace('&', 'and')\n",
    "            else:\n",
    "                page_title2 = page_title\n",
    "            url = (\n",
    "                'https://en.wikipedia.org/w/api.php?action=query&prop=info&inprop=subjectid&titles={}&format=json'.format(page_title2))\n",
    "            json_response = requests.get(url).json()\n",
    "            wiki_page_id  = [page_id for page_id, page_info in json_response['query']['pages'].items()][0]\n",
    "\n",
    "            #pull wiki page\n",
    "            movie_title = np.nan\n",
    "            movie_id    = np.nan\n",
    "\n",
    "            try:\n",
    "                movie_title = wikipedia.page(page_title2, auto_suggest=False, redirect=True)\n",
    "            except:\n",
    "                movie_id = wikipedia.page(pageid=wiki_page_id)\n",
    "\n",
    "            try:\n",
    "                movie_id = wikipedia.page(pageid=wiki_page_id)\n",
    "            except:\n",
    "                movie_title = wikipedia.page(page_title2, auto_suggest=False, redirect=True)\n",
    "\n",
    "            if str(movie_title) != 'nan':\n",
    "                movie = movie_title\n",
    "            elif str(movie_id) != 'nan':\n",
    "                movie = movie_id\n",
    "            else:\n",
    "                print(\"can\\'t find url\")\n",
    "                continue\n",
    "\n",
    "            movie_url = movie.url\n",
    "            print(movie_url)\n",
    "\n",
    "            #scraping\n",
    "\n",
    "            # sending the request\n",
    "            response = requests.get(movie_url)\n",
    "\n",
    "            # parsing the response\n",
    "            soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "            if 'MOVIE' in str(soup).upper():\n",
    "                try:\n",
    "                    output_list = wiki_data_scrape(movie_name, soup, movie_url)\n",
    "                    temp_df.loc[len(temp_df)] = output_list\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                print('url not for movie!')\n",
    "\n",
    "    #merge ratings df and scraped df\n",
    "    new_df = pd.merge(new_df, temp_df, on='Title', how='inner')\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
